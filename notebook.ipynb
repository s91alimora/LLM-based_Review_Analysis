{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bf7ab53-85f2-4300-965e-5502420335f8",
   "metadata": {},
   "source": [
    "## Importing Data & Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "bd799691-8dca-4b69-b92d-fb4b59ab14a9",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 52,
    "lastExecutedAt": 1743793068279,
    "lastExecutedByKernel": "2628eeb1-906f-4c8a-a651-f3de7e20ba0a",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from transformers import pipeline\nimport evaluate\nimport re\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForQuestionAnswering"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import evaluate\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForQuestionAnswering\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import logging\n",
    "logging.set_verbosity(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "8cc39816-dabf-4bb5-881a-c52da16e0e6e",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastExecutedByKernel": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "chartState": {
       "chartModel": {
        "cellRange": {
         "columns": [],
         "rowEndIndex": null,
         "rowEndPinned": null,
         "rowStartIndex": null,
         "rowStartPinned": null
        },
        "chartId": "id-cs0e6zrvs1c",
        "chartOptions": {
         "common": {
          "animation": {
           "enabled": true
          }
         }
        },
        "chartPalette": {
         "altDown": {
          "fill": "#ffa03a",
          "stroke": "#cc6f10"
         },
         "altNeutral": {
          "fill": "#b5b5b5",
          "stroke": "#575757"
         },
         "altUp": {
          "fill": "#5090dc",
          "stroke": "#2b5c95"
         },
         "down": {
          "fill": "#ef5452",
          "stroke": "#a82529"
         },
         "fills": [
          "#6568A0",
          "#43D7A4",
          "#4095DB",
          "#FACC5F",
          "#CAE279",
          "#F08083",
          "#5BCDF2",
          "#F099DC",
          "#965858",
          "#7DB64F",
          "#A98954"
         ],
         "neutral": {
          "fill": "#b5b5b5",
          "stroke": "#575757"
         },
         "strokes": [
          "#6568A0",
          "#43D7A4",
          "#4095DB",
          "#FACC5F",
          "#CAE279",
          "#F08083",
          "#5BCDF2",
          "#F099DC",
          "#965858",
          "#7DB64F",
          "#A98954"
         ],
         "up": {
          "fill": "#459d55",
          "stroke": "#1e652e"
         }
        },
        "chartThemeName": "datalabTheme",
        "chartType": "groupedColumn",
        "modelType": "range",
        "suppressChartRanges": false,
        "switchCategorySeries": false,
        "unlinkChart": false,
        "version": "32.2.2"
       },
       "rangeChartModel": {
        "rangeColumns": [],
        "switchCategorySeries": false
       }
      },
      "height": 501,
      "tableState": {
       "customFilter": {
        "const": {
         "type": "boolean",
         "valid": true,
         "value": true
        },
        "id": "c068f983-6568-4222-8c10-e835d1249518",
        "nodeType": "const"
       }
      },
      "type": "dataFrame"
     }
    },
    "version": "ag-charts-v1",
    "visualizeDataframe": false
   },
   "outputs": [
    {
     "data": {
      "application/com.datacamp.data-table.v2+json": {
       "table": {
        "data": {
         "Class": [
          "POSITIVE",
          "NEGATIVE",
          "POSITIVE",
          "NEGATIVE",
          "POSITIVE"
         ],
         "Review": [
          "I am very satisfied with my 2014 Nissan NV SL. I use this van for my business deliveries and personal use. Camping, road trips, etc. We dont have any children so I store most of the seats in my warehouse. I wanted the passenger van for the rear air conditioning. We drove our van from Florida to California for a Cross Country trip in 2014. We averaged about 18 mpg. We drove thru a lot of rain and It was a very comfortable and stable vehicle. The V8 Nissan Titan engine is a 500k mile engine. It has been tested many times by delivery and trucking companies. This is why Nissan gives you a 5 year or 100k mile bumper to bumper warranty. Many people are scared about driving this van because of its size. But with front and rear sonar sensors, large mirrors and the back up camera. It is easy to drive. The front and rear sensors also monitor the front and rear sides of the bumpers making it easier to park close to objects. Our Nissan NV is a Tow Monster. It pulls our 5000 pound travel trailer like its not even there. I have plenty of power to pass a vehicle if needed. The 5.6 liter engine produces 317 hp. I have owned Chevy and Ford vans and there were not very comfortable and had little cockpit room. The Nissan NV is the only vehicle made that has the engine forward like a pick up truck giving the driver plenty of room and comfort in the cockpit area. I dont have any negatives to say about my NV. This is a wide vehicle. The only modification I would like to see from Nissan is for them to add amber side mirror marker lights.BTW. I now own a 2016 Nissan NVP SL. Love it.",
          "The car is fine. It's a bit loud and not very powerful. On one hand, compared to its peers, the interior is well-built. The transmission failed a few years ago, and the dealer replaced it under warranty with no issues. Now, about 60k miles later, the transmission is failing again. It sounds like a truck, and the issues are well-documented. The dealer tells me it is normal, refusing to do anything to resolve the issue. After owning the car for 4 years, there are many other vehicles I would purchase over this one. Initially, I really liked what the brand is about: ride quality, reliability, etc. But I will not purchase another one. Despite these concerns, I must say, the level of comfort in the car has always been satisfactory, but not worth the rest of issues found.",
          "My first foreign car. Love it, I would buy another.",
          "I've come across numerous reviews praising the Rogue, and I genuinely feel like I might be missing something. It's only been a week since I got the car, and I am genuinely disappointed. I truly wish I could return it. My main concern revolves around what I see as a significant design flaw (which I believe also exists in the Murano, though that wasn't much better and considerably pricier). The rear windshield is just too small. The headrests in the back seat obstruct the sides of the rearview window. This \"Crossover\" feels more like a cheaply made compact car. My other vehicle is a Sonata, and it provides a significantly quieter and smoother ride. I did not anticipate this car to ride so roughly; my 2006 Pathfinder had a smoother ride! I would rate this car a 5 all around.",
          "I've been dreaming of owning an SUV for quite a while, but I've been driving cars that were already paid for during an extended period. I ultimately made the decision to transition to a brand-new car, which, of course, involved taking on new payments. However, given that I don't drive extensively, I was inclined to avoid a substantial financial commitment. The Nissan Rogue provides me with the desired SUV experience without burdening me with an exorbitant payment; the financial arrangement is quite reasonable. Handling and styling are great; I have hauled 12 bags of mulch in the back with the seats down and could have held more. I am VERY satisfied overall. I find myself needing to exercise extra caution when making lane changes, particularly owing to the blind spots resulting from the small side windows situated towards the rear of the vehicle. To address this concern, I am actively engaged in making adjustments to my mirrors and consciously reducing the frequency of lane changes. The engine delivers strong performance, and the ride is really smooth."
         ],
         "index": [
          0,
          1,
          2,
          3,
          4
         ]
        },
        "schema": {
         "fields": [
          {
           "name": "index",
           "type": "integer"
          },
          {
           "name": "Review",
           "type": "string"
          },
          {
           "name": "Class",
           "type": "string"
          }
         ],
         "pandas_version": "1.4.0",
         "primaryKey": [
          "index"
         ]
        }
       },
       "total_rows": 5,
       "truncation_type": null
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am very satisfied with my 2014 Nissan NV SL....</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The car is fine. It's a bit loud and not very ...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My first foreign car. Love it, I would buy ano...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've come across numerous reviews praising the...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I've been dreaming of owning an SUV for quite ...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review     Class\n",
       "0  I am very satisfied with my 2014 Nissan NV SL....  POSITIVE\n",
       "1  The car is fine. It's a bit loud and not very ...  NEGATIVE\n",
       "2  My first foreign car. Love it, I would buy ano...  POSITIVE\n",
       "3  I've come across numerous reviews praising the...  NEGATIVE\n",
       "4  I've been dreaming of owning an SUV for quite ...  POSITIVE"
      ]
     },
     "execution_count": 189,
     "metadata": {
      "application/com.datacamp.data-table.v2+json": {
       "status": "success"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_reviews = pd.read_csv('path/car_reviews.csv', sep=\";\")\n",
    "car_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad68d19d-eeab-4e17-9631-7f35f36a10f4",
   "metadata": {},
   "source": [
    "# Step 1: Classifying Car Reviews\n",
    "\n",
    "Use a pre-trained LLM to classify the sentiment of the five car reviews in the car_reviews.csv dataset, and evaluate the classification accuracy and F1 score of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ec02ab-914e-4523-ae08-66f46d753931",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "0793f3a6-30e4-47cd-a390-e4ff6038de4b",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 181,
    "lastExecutedAt": 1743793068460,
    "lastExecutedByKernel": "2628eeb1-906f-4c8a-a651-f3de7e20ba0a",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Load a sentiment analysis pipeline (this model returns \"POSITIVE\" or \"NEGATIVE\")\nsentiment_pipeline = pipeline(\"sentiment-analysis\")",
    "outputsMetadata": {
     "0": {
      "height": 101,
      "type": "stream"
     },
     "5": {
      "height": 38,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Load a sentiment analysis pipeline (this model returns \"POSITIVE\" or \"NEGATIVE\")\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "762f0884-f9bc-4fb5-b974-84f9815076a2",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 280,
    "lastExecutedAt": 1743793068740,
    "lastExecutedByKernel": "2628eeb1-906f-4c8a-a651-f3de7e20ba0a",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Process all reviews in the dataset\npredicted_labels = sentiment_pipeline(car_reviews[\"Review\"].tolist())\npredicted_labels",
    "outputsMetadata": {
     "0": {
      "height": 59,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.929397702217102},\n",
       " {'label': 'POSITIVE', 'score': 0.8654273152351379},\n",
       " {'label': 'POSITIVE', 'score': 0.9994640946388245},\n",
       " {'label': 'NEGATIVE', 'score': 0.9935314059257507},\n",
       " {'label': 'POSITIVE', 'score': 0.9986565113067627}]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process all reviews in the dataset\n",
    "predicted_labels = sentiment_pipeline(car_reviews[\"Review\"].tolist())\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "fda71e53-ebb1-40b2-8377-2a4d8def520f",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 47,
    "lastExecutedAt": 1743793068787,
    "lastExecutedByKernel": "2628eeb1-906f-4c8a-a651-f3de7e20ba0a",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Mapping predicted labels to binary values: 1 for POSITIVE, 0 for NEGATIVE\npredictions = [1 if pred[\"label\"].upper() == \"POSITIVE\" else 0 for pred in predicted_labels]\npredictions"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 0, 1]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mapping predicted labels to binary values: 1 for POSITIVE, 0 for NEGATIVE\n",
    "predictions = [1 if pred[\"label\"].upper() == \"POSITIVE\" else 0 for pred in predicted_labels]\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "542557cc-4e5c-4173-831c-668ac225924a",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 47,
    "lastExecutedAt": 1743793068834,
    "lastExecutedByKernel": "2628eeb1-906f-4c8a-a651-f3de7e20ba0a",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Mapping the reference classes in the dataset to binary as well\nref_labels = [1 if label.upper() == \"POSITIVE\" else 0 for label in car_reviews[\"Class\"].tolist()]\nref_labels"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 0, 1]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mapping the reference classes in the dataset to binary as well\n",
    "ref_labels = [1 if label.upper() == \"POSITIVE\" else 0 for label in car_reviews[\"Class\"].tolist()]\n",
    "ref_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d70ce07-12c4-4b8c-85d7-f49118be9290",
   "metadata": {},
   "source": [
    "## Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "1fdfe15e-9878-4a9d-800a-e20c6d884ec0",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 338,
    "lastExecutedAt": 1743793069172,
    "lastExecutedByKernel": "2628eeb1-906f-4c8a-a651-f3de7e20ba0a",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Accuracy of the model\naccuracy_metric = evaluate.load(\"accuracy\")\naccuracy_result = accuracy_metric.compute(predictions=predictions, references=ref_labels)[\"accuracy\"]\nprint(\"Accuracy:\", accuracy_result)\n\n#F1 Score for the model\nf1_metric = evaluate.load(\"f1\")\nf1_result = f1_metric.compute(predictions=predictions, references=ref_labels, average=\"binary\")[\"f1\"]\nprint(\"\\nF1 Score:\", f1_result)",
    "outputsMetadata": {
     "0": {
      "height": 80,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8\n",
      "\n",
      "F1 Score: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of the model\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "accuracy_result = accuracy_metric.compute(predictions=predictions, references=ref_labels)[\"accuracy\"]\n",
    "print(\"Accuracy:\", accuracy_result)\n",
    "\n",
    "#F1 Score for the model\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "f1_result = f1_metric.compute(predictions=predictions, references=ref_labels, average=\"binary\")[\"f1\"]\n",
    "print(\"\\nF1 Score:\", f1_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d91408b-a93a-478a-a2a1-9bdda61af365",
   "metadata": {},
   "source": [
    "# Step 2: Translate a Car Review\n",
    "\n",
    "The company is recently attracting customers from Spain. Extract and pass the _first two sentences_ of the first review in the dataset to an English-to-Spanish translation LLM. Calculate the BLEU score to assess translation quality, using the content in `reference_translations.txt` as references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "7d6d5cd8-15e3-43fe-ab4e-21a1d9d6ad58",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 47,
    "lastExecutedAt": 1743793069219,
    "lastExecutedByKernel": "2628eeb1-906f-4c8a-a651-f3de7e20ba0a",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Extracting the first two sentences from the first review\nfirst_two_sentences = \" \".join(re.split(r'(?<=[.!?])\\s+', car_reviews.Review[0])[:2])\nfirst_two_sentences"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am very satisfied with my 2014 Nissan NV SL. I use this van for my business deliveries and personal use.'"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting the first two sentences from the first review\n",
    "first_two_sentences = \" \".join(re.split(r'(?<=[.!?])\\s+', car_reviews.Review[0])[:2])\n",
    "first_two_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "23f7fd84-a1bf-43cd-96c0-19a88b5ab8d2",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 2350,
    "lastExecutedAt": 1743793071569,
    "lastExecutedByKernel": "2628eeb1-906f-4c8a-a651-f3de7e20ba0a",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Load the model and tokenizer for English-to-Spanish translation\ntokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\")"
   },
   "outputs": [],
   "source": [
    "# Load the model and tokenizer for English-to-Spanish translation\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "72a12bb3-3b4a-4af4-a871-957b86c08be8",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 627,
    "lastExecutedAt": 1743793072197,
    "lastExecutedByKernel": "2628eeb1-906f-4c8a-a651-f3de7e20ba0a",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# tokanizing the first two sentences\nnew_input = tokenizer(first_two_sentences, return_tensors=\"pt\")\n\n# Generate translation with torch.no_grad() to avoid computing gradients.\nwith torch.no_grad():\n    outputs = model.generate(\n        new_input[\"input_ids\"],\n        max_length=128,  # set maximum tokens to generate\n        eos_token_id=tokenizer.eos_token_id,  # tell the model when to stop by using the end-of-sequence token\n        early_stopping=True  # optional: stop early if possible\n    )\n    \ntranslated_review = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(\"Translated review:\", translated_review)",
    "outputsMetadata": {
     "0": {
      "height": 38,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated review: Estoy muy satisfecho con mi Nissan NV SL 2014. Uso esta camioneta para mis entregas de negocios y uso personal.\n"
     ]
    }
   ],
   "source": [
    "# tokanizing the first two sentences\n",
    "new_input = tokenizer(first_two_sentences, return_tensors=\"pt\")\n",
    "\n",
    "# Generate translation with torch.no_grad() to avoid computing gradients.\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        new_input[\"input_ids\"],\n",
    "        max_length=128,  # set maximum tokens to generate\n",
    "        eos_token_id=tokenizer.eos_token_id,  # tell the model when to stop by using the end-of-sequence token\n",
    "        early_stopping=True  # optional: stop early if possible\n",
    "    )\n",
    "    \n",
    "translated_review = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Translated review:\", translated_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "35d3e7bd-5dcd-4c23-885e-b0b649d83dfc",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 51,
    "lastExecutedAt": 1743793072248,
    "lastExecutedByKernel": "2628eeb1-906f-4c8a-a651-f3de7e20ba0a",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Read the reference translation from file\nwith open(\"data/reference_translations.txt\", \"r\", encoding=\"utf-8\") as f:\n    ref_translation = f.read().splitlines()\n    \nref_translation"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Estoy muy satisfecho con mi Nissan NV SL 2014. Utilizo esta camioneta para mis entregas comerciales y uso personal.',\n",
       " 'Estoy muy satisfecho con mi Nissan NV SL 2014. Uso esta furgoneta para mis entregas comerciales y uso personal.']"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the reference translation from file\n",
    "with open(\"data/reference_translations.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ref_translation = f.read().splitlines()\n",
    "    \n",
    "ref_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "db5d3f4d-ba1d-458f-894e-e24698b24830",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1438,
    "lastExecutedAt": 1743793073686,
    "lastExecutedByKernel": "2628eeb1-906f-4c8a-a651-f3de7e20ba0a",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Calculate the BLEU score for two references\nbleu = evaluate.load(\"bleu\")\nbleu_score = bleu.compute(predictions=[translated_review], references=[ref_translation[0]])\nprint(\"BLEU score\", bleu_score[\"bleu\"])",
    "outputsMetadata": {
     "0": {
      "height": 38,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score 0.6888074582865503\n"
     ]
    }
   ],
   "source": [
    "# Calculate the BLEU score for two references\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "bleu_score = bleu.compute(predictions=[translated_review], references=[ref_translation[0]])\n",
    "print(\"BLEU score\", bleu_score[\"bleu\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebcc44f-9a5a-4e38-aa3c-42bf682f739e",
   "metadata": {},
   "source": [
    "# Step 3: Ask a question about a car review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b9c2a6-d77c-473f-953d-ff6a292253a5",
   "metadata": {},
   "source": [
    "The 2nd review in the dataset emphasizes brand aspects. Load an extractive QA LLM such as `\"deepset/minilm-uncased-squad2\"` to formulate the question `\"What did he like about the brand?\"` and obtain an answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d25279-3062-451a-8fad-dff802f7099a",
   "metadata": {},
   "source": [
    "### Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "19c9bf97-04ea-418f-9a41-be6bf72a3046",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastExecutedByKernel": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 164,
      "type": "stream"
     },
     "1": {
      "height": 206,
      "type": "stream"
     },
     "2": {
      "height": 143,
      "type": "stream"
     },
     "6": {
      "height": 38,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/minilm-uncased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3: Extractive QA\n",
      "Question: What did he like about the brand?\n",
      "Context (2nd review): The car is fine. It's a bit loud and not very powerful. On one hand, compared to its peers, the interior is well-built. The transmission failed a few years ago, and the dealer replaced it under warranty with no issues. Now, about 60k miles later, the transmission is failing again. It sounds like a truck, and the issues are well-documented. The dealer tells me it is normal, refusing to do anything to resolve the issue. After owning the car for 4 years, there are many other vehicles I would purchase over this one. Initially, I really liked what the brand is about: ride quality, reliability, etc. But I will not purchase another one. Despite these concerns, I must say, the level of comfort in the car has always been satisfactory, but not worth the rest of issues found.\n",
      "Answer: ride quality, reliability\n"
     ]
    }
   ],
   "source": [
    "# Load an extractive QA pipeline with a model such as deepset/minilm-uncased-squad2\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/minilm-uncased-squad2\")\n",
    "\n",
    "# Define the question and use the 2nd review (index 1) as context\n",
    "question = \"What did he like about the brand?\"\n",
    "context = car_reviews[\"Review\"].iloc[1]\n",
    "\n",
    "qa_output = qa_pipeline(question=question, context=context)\n",
    "answer = qa_output[\"answer\"]\n",
    "\n",
    "print(\"\\nStep 3: Extractive QA\")\n",
    "print(\"Question:\", question)\n",
    "print(\"Context (2nd review):\", context)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c360d-8619-4bbe-b52b-9d4e3d5a403d",
   "metadata": {},
   "source": [
    "### Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "553e5e7f-0225-4a3b-8c3f-65713f2f01d9",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 312,
    "lastExecutedAt": 1743793074281,
    "lastExecutedByKernel": "2628eeb1-906f-4c8a-a651-f3de7e20ba0a",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Define model name and load the tokenizer and model using Auto classes\nmodel_name = \"deepset/minilm-uncased-squad2\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\n\n# Define the question and extract the context (2nd review from the dataframe)\nquestion = \"What did he like about the brand?\"\ncontext = car_reviews.Review.iloc[1]\n\n# Tokenize the input: the tokenizer will handle concatenating the question and context\ninputs = tokenizer(question, context, return_tensors=\"pt\")\n\n# Perform inference\nwith torch.no_grad():\n    outputs = model(**inputs)\n    \n# The model outputs two sets of logits for start and end positions.\nstart_logits = outputs.start_logits\nend_logits = outputs.end_logits\n\n# Identify the most likely start and end positions for the answer\nstart_index = torch.argmax(start_logits)\nend_index = torch.argmax(end_logits) + 1\n\n# Extract the tokens corresponding to the answer span\nanswer_tokens = inputs[\"input_ids\"][0][start_index:end_index]\nraw_answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n\n# Post-processing: Clean up the answer text (e.g., strip whitespace)\nanswer = raw_answer.strip()\n\nprint(\"Question:\", question)\nprint(\"Context:\", context)\nprint(\"Extracted Answer:\", answer)",
    "outputsMetadata": {
     "0": {
      "height": 143,
      "type": "stream"
     },
     "1": {
      "height": 164,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/minilm-uncased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What did he like about the brand?\n",
      "Context: The car is fine. It's a bit loud and not very powerful. On one hand, compared to its peers, the interior is well-built. The transmission failed a few years ago, and the dealer replaced it under warranty with no issues. Now, about 60k miles later, the transmission is failing again. It sounds like a truck, and the issues are well-documented. The dealer tells me it is normal, refusing to do anything to resolve the issue. After owning the car for 4 years, there are many other vehicles I would purchase over this one. Initially, I really liked what the brand is about: ride quality, reliability, etc. But I will not purchase another one. Despite these concerns, I must say, the level of comfort in the car has always been satisfactory, but not worth the rest of issues found.\n",
      "Extracted Answer: ride quality, reliability\n"
     ]
    }
   ],
   "source": [
    "# Define model name and load the tokenizer and model using Auto classes\n",
    "model_name = \"deepset/minilm-uncased-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Define the question and extract the context (2nd review from the dataframe)\n",
    "question = \"What did he like about the brand?\"\n",
    "context = car_reviews.Review.iloc[1]\n",
    "\n",
    "# Tokenize the input: the tokenizer will handle concatenating the question and context\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "# The model outputs two sets of logits for start and end positions.\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "# Identify the most likely start and end positions for the answer\n",
    "start_index = torch.argmax(start_logits)\n",
    "end_index = torch.argmax(end_logits) + 1\n",
    "\n",
    "# Extract the tokens corresponding to the answer span\n",
    "answer_tokens = inputs[\"input_ids\"][0][start_index:end_index]\n",
    "raw_answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
    "\n",
    "# Post-processing: Clean up the answer text (e.g., strip whitespace)\n",
    "answer = raw_answer.strip()\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Context:\", context)\n",
    "print(\"Extracted Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb57994-d75e-47ad-898a-e6ce184ff038",
   "metadata": {},
   "source": [
    "# Step 4: Summarize and analyze a car review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483e04ca-7b3a-4140-96c3-f06c06658fa4",
   "metadata": {},
   "source": [
    "Summarize the last review in the dataset, into approximately 50-55 tokens long. Store it in the variable `summarized_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "bc9ae175-9c8b-47e7-83d9-efe96ee630c6",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 21457,
    "lastExecutedAt": 1743793095738,
    "lastExecutedByKernel": "2628eeb1-906f-4c8a-a651-f3de7e20ba0a",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Load a summarization pipeline (using a model like facebook/bart-large-cnn)\nsummarization_pipeline = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n\n# Extract the last car review from the dataset\nlast_review = car_reviews.Review.iloc[-1]\n\n# Generate a summary with approximately 50-55 tokens.\nsummarized_output = summarization_pipeline(last_review, min_length=50, max_length=55, do_sample=False)\nsummarized_text = summarized_output[0][\"summary_text\"]\n\n# Print the generated summary\nprint(\"Summarized Text:\", summarized_text)\n\n# Load the bias evaluation metrics (toxicity and regard) from the evaluate library\ntoxicity_metric = evaluate.load(\"toxicity\")\nregard_metric = evaluate.load(\"regard\")\n\n# Format the summarized text as a list for the metric inputs\ntoxicity_result = toxicity_metric.compute(predictions=[summarized_text], aggregation = 'maximum')\nregard_result = regard_metric.compute(data=[summarized_text])\n\nprint(\"Toxicity:\", toxicity_result['max_toxicity'])\nprint(\"\\n Regard scores for each lable are as following:\")\nregard_df = pd.DataFrame(regard_result['regard'][0])\nregard_df.set_index('label', inplace=True)\nregard_df",
    "outputsMetadata": {
     "0": {
      "height": 38,
      "type": "stream"
     },
     "1": {
      "height": 59,
      "type": "stream"
     },
     "2": {
      "height": 59,
      "type": "stream"
     },
     "3": {
      "height": 80,
      "type": "stream"
     },
     "4": {
      "height": 501,
      "tableState": {
       "customFilter": {
        "const": {
         "type": "boolean",
         "valid": true,
         "value": true
        },
        "id": "c068f983-6568-4222-8c10-e835d1249518",
        "nodeType": "const"
       }
      },
      "type": "dataFrame"
     },
     "6": {
      "height": 38,
      "type": "stream"
     },
     "7": {
      "height": 59,
      "type": "stream"
     },
     "9": {
      "height": 38,
      "type": "stream"
     },
     "16": {
      "height": 38,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized Text: The Nissan Rogue provides me with the desired SUV experience without burdening me with an exorbitant payment. Handling and styling are great; I have hauled 12 bags of mulch in the back with the seats down and could have held more. The engine delivers strong\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity: 0.00013863427739124745\n",
      "\n",
      " Regard scores for each lable are as following:\n"
     ]
    },
    {
     "data": {
      "application/com.datacamp.data-table.v2+json": {
       "table": {
        "data": {
         "label": [
          "positive",
          "neutral",
          "other",
          "negative"
         ],
         "score": [
          0.6263338923,
          0.2027347684,
          0.1229157522,
          0.0480155796
         ]
        },
        "schema": {
         "fields": [
          {
           "name": "label",
           "type": "string"
          },
          {
           "name": "score",
           "type": "number"
          }
         ],
         "pandas_version": "1.4.0",
         "primaryKey": [
          "label"
         ]
        }
       },
       "total_rows": 4,
       "truncation_type": null
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>0.626334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>0.202735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other</th>\n",
       "      <td>0.122916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>0.048016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             score\n",
       "label             \n",
       "positive  0.626334\n",
       "neutral   0.202735\n",
       "other     0.122916\n",
       "negative  0.048016"
      ]
     },
     "execution_count": 203,
     "metadata": {
      "application/com.datacamp.data-table.v2+json": {
       "status": "success"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a summarization pipeline (using a model like facebook/bart-large-cnn)\n",
    "summarization_pipeline = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Extract the last car review from the dataset\n",
    "last_review = car_reviews.Review.iloc[-1]\n",
    "\n",
    "# Generate a summary with approximately 50-55 tokens.\n",
    "summarized_output = summarization_pipeline(last_review, min_length=50, max_length=55, do_sample=False)\n",
    "summarized_text = summarized_output[0][\"summary_text\"]\n",
    "\n",
    "# Print the generated summary\n",
    "print(\"Summarized Text:\", summarized_text)\n",
    "\n",
    "# Load the bias evaluation metrics (toxicity and regard) from the evaluate library\n",
    "toxicity_metric = evaluate.load(\"toxicity\")\n",
    "regard_metric = evaluate.load(\"regard\")\n",
    "\n",
    "# Format the summarized text as a list for the metric inputs\n",
    "toxicity_result = toxicity_metric.compute(predictions=[summarized_text], aggregation = 'maximum')\n",
    "regard_result = regard_metric.compute(data=[summarized_text])\n",
    "\n",
    "print(\"Toxicity:\", toxicity_result['max_toxicity'])\n",
    "print(\"\\n Regard scores for each lable are as following:\")\n",
    "regard_df = pd.DataFrame(regard_result['regard'][0])\n",
    "regard_df.set_index('label', inplace=True)\n",
    "regard_df"
   ]
  }
 ],
 "metadata": {
  "editor": "DataLab",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
